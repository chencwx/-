# 基础文献11-Deep learning[M]

>[11]GOODFELLOW I, BENGIO Y, COURVILLE A, et al. Deep learning[M]. Cambridge: The
MIT Press, 2016.

- 本书可分为三部分：
  - 第一部分介绍基本的数学工具和机器学习的概念；
  - 第二部分介绍最成熟的深度学习算法，这些技术基本上已经得到解决；
  - 第三部分讨论某些具有展望性的想法，它们被广泛地认为是深度学习未来的研究重点。

## 科学问题

- 本书讨论一种解决方案，该方案可以让计算机从经验中学习，并根据层次化的概念体系来理解世界，而每个概念则通过与某些相对简单的概念之间的关系来定义。
- AI 系统需要具备自己获取知识的能力，即从原始数据中提取模式的能力，这种能力被称为机器学习（machine learning）。
- 表示的选择会对机器学习算法的性能产生巨大的影响。许多人工智能任务都可以通过以下方式解决：先提取一个合适的特征集，然后将这些特征提供给简单的机器学习算法。
- 对于许多问题来说，我们很难知道应该提取哪些特征，表示学习（representation learning）是使用机器学习来发掘表示本身的方法之一。

## 基础理论

- 机器学习定义：对于某类任务 T 和性能度量 P，一个计算机程序被认为可以从经验 E 中学习是指，通过经验 E 改进后，它在任务 T 上由性能度量 P衡量的性能有所提升。
- 无监督学习算法训练含有很多特征的数据集，然后学习出这个数据集上有用的结构性质。 在深度学习中，我们通常要学习生成数据集的整个概率分布。还有一些其他类型的无监督学习任务，例如聚类，会将数据集分成相似样本的集合。
- 监督学习算法训练含有很多特征的数据集，不过数据集中的样本都有一个标签或目标。
- 欠拟合是指模型不能在训练集上获得足够低的误差。 而过拟合是指训练误差和和测试误差之间的差距太大。

## 技术框架/关键技术

- 已经在实践中被大规模应用的深度学习都是基于监督学习这种方式。
- 通过添加更多层以及向层内添加更多单元，深度网络可以表示复杂性不断增加的函数。
- 给定足够大的模型和足够大的标注训练数据集，可以通过深度学习将输入向量映射到输出向量，完成大多数对人来说能迅速处理的任务。
- **深度前馈网络**本质上是一个函数近似器。
- 前馈网络模型常常会采用多层函数进行拟合，从而得到类似于这样的模型。这个函数链的全长就叫做模型的深度，其中函数叫做函数的输出层。最内层的的输入十分容易确定，但是中间的其他函数层的输入和输出都暂时无从得知，故称为隐藏层。
- **整流线性单元(ReLU)**，拥有一些优美的性质，比sigmoid函数更适合当隐藏单元，其激活函数为:g(z)=max{0,z}
- 代价函数(cost function)，通过变分法得到的平均绝对误差和均方误差都不能很好的适用于基于梯度的优化方法，如当某个神经元趋于饱和，它的梯度将会很小很小，因此一般采用交叉熵代价函数。交叉熵代价函数可以避免上述的梯度饱和问题，并且能在我们预测错误时都能有一个较大的代价。
- **正则化和优化**这种模型的高级技术将这些模型扩展到大输入（如高分辨率图像或长时间序列）需要专门化。

## 研究方法

- 当设计特征或用于学习特征的算法时，我们的目标通常是分离出能解释观察数据的变差因素（factors of variation）。
- 深度学习（deep learning）通过其他较简单的表示来表达复杂表示，解决了表示学习中的核心问题。
- 深度学习基本模型：
  - 可见层（visible layer），它包含我们能观察到的变量。
  - 隐藏层（hidden layer）， 它们的值不在数据中给出。
  - 输出层（output layer），识别图像中存在的对象。
- 目前主要有两种度量模型深度的方式：
  - 一种是基于评估架构所需执行的顺序指令的数目。深度是从输入到输出的最长路径的长度，但这取决于可能的计算步骤的定义。
  - 另一种是将描述概念彼此如何关联的图的深度视为模型深度。
- 韦恩图表示深度学习地位：（（（（深度学习）表示学习）机器学习）AI）

## 前沿进展/研究进展

- 深度学习是表示学习中的一种，在它的基础上多了这一部分：**自动提取特征**。
- 神经网络的发展分为几个阶段，第一个是单层网络，第二个两层，第三个多层。这三层网络在激活函数、异或问题和复杂问题上有所不同。
- 单层网络只有两种状态，激活或者抑制，是一个函数值是1或者-1的符号函数，无法解决异或函数和复杂问题。
- 两层网络的结构更复杂，激活函数不再是符号函数，而是sigmoid，可以解决异或问题，但是解决不了复杂问题。
- 多层网络，也就是现在的深度学习，激活函数主要是ReLU，可以解决异或问题和复杂问题。
- 深度学习知名的网络结构主要两种：
  - 第一种是前馈，信息从一层一层往下流动，一路向前不回头（不算误差反向传播）。以CNN为例；
  - 第二种是反馈，即把上一次训练的状态留下来，作为本次的一个输入，它是前馈的一种扩展，特点是训练时会把上一次训练结果拿过来用，再决定下一步的训练。典型例子就是RNN。

## 创新建议

- 书中已经介绍了如何解决监督学习问题，即在给定足够的映射样本的情况下，学习将一个向量映射到另一个。但是想要解决的问题并不全都属于这个类别。我们可能希望生成新的样本、或确定一个点的似然性、或处理缺失值以及利用一组大量的未标记样本或相关任务的样本。
- 当前应用于工业的最先进技术的缺点是我们的学习算法需要**大量的监督数据**才能实现良好的精度。实现这些目标通常需要某种形式的无监督或半监督学习。
- 无监督学习困难的核心原因是被建模的随机变量的高维度。这带来了两个不同的挑战：统计挑战和计算挑战。
  - 计算挑战来自执行难解的推断或难解的归一化分布，难解的推断主要在第十九章近似推断中讨论；难解的归一化常数通过第十八章讨论的配分函数来解决，第十七章介绍的马尔可夫链蒙特卡罗方法通常用来处理配分函数。上面这些方法的核心思想是面对这些难以处理的计算，通过近似它们的方式来处理。
  - 而解决计算挑战的另一种有趣的方式是通过设计模型，完全避免这些难以处理的计算，因此不需要这些计算的方法是非常有吸引力的。
